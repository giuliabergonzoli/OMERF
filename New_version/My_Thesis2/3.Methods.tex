%-----------------------------------------------------------------------------
% EQUATIONS
%-----------------------------------------------------------------------------
\section{Model and Methods}
\label{sec:methods}
In this section, a concise overview of cumulative link models for ordinal categorical data (Section \ref{sec:clm} and \ref{sec:clmm}) and random forest (Section \ref{sec:rf}) is provided.
This will serve as the foundation for the explanation and understanding of the OMERF method and its algorithm detailed in Section \ref{sec:omerf}.  Section \ref{sec:gof} is devoted to inspect different performance measures to evaluate the Goodness of Fit of ordinal models.
These measures serve as assessment criteria for the compared models.

\subsection{Background and state of the art models}

\subsubsection{Cumulative link models}
\label{sec:clm}
A Cumulative Link Model (CLM) \cite{agresti2010analysis,ananth1997regression,mccullagh1980regression} is an analytical framework to deal with observations on an ordinal scale.
These models applied to ordinal responses are advantageous in the fact that there is no need to define interval-scale assumptions about distances between response categories.

Ordinal observations can be expressed through a random variable, \(y_{j}\), which takes on the value \(c\) when the \(j\)-th ordinal observation falls within the \(c\)-th category. \(c\) takes integer values in the range from 1 to \(C\), where \(C\) is greater than or equal to 2.

The linear predictor of CLMs can be expressed as follows:
\begin{equation}
    \label{clm}
    \begin{aligned}
        \eta_{jc} &= g(\gamma_{jc}) = \theta_{c} - \bm{x}_{j}^T \bm{\beta}, \qquad  j=1,\dots,J \qquad c=1,\dots,C-1
    \end{aligned}
\end{equation}
where \(\gamma_{jc} = \mathbb{P}(y_{j} \leq c) = \pi_{j1} + \dots + \pi_{jc}\) (with \(\sum_{c=1}^{C} \pi_{jc}=1\)) are cumulative probabilities, \(\pi_{jc}\) is the probability that the \(j\)-th observation falls in the \(c\)-th category, \(\eta_{jc}\) is the linear predictor
and \(\bm{x}_{j}\) is a \(p\)-dimensional vector of fixed effects regression variables, to which corresponds the parameter vector \(\bm{\beta}\).
Finally, \(g\) is the monotonic, differentiable link function and \(\theta_{c}\) are the strictly ordered thresholds (also known as cut-points or intercepts):
\begin{gather*}
    \begin{aligned}
        -\infty \equiv \theta_{0} \leq \theta_{1} \leq \dots \leq \theta_{C-1} \leq \theta_{C} \equiv \infty.
    \end{aligned}
\end{gather*}


\subsubsection{Cumulative link mixed models}
\label{sec:clmm}
Cumulative Link Mixed Models (CLMM) \cite{grilli2011multilevel,tutz1996random} are the multilevel extension of cumulative regression models with normally distributed random effects for ordinal responses. These models are suited to handle data with a hierarchical structure: given \(\bm{y}_{i}\) = \(y_{i1}, \dots, y_{in_{i}}\) the \(n_{i}\)-dimensional response vector for observations in the \(i\)-th group, its elements \(y_{ij}\) are supposed to be conditionally independent on the random effects \(\bm{b}_{i}\).
The standard assumption on the random effects \(\bm{b}_{i}\) is that, conditionally on the fixed covariates, they are independent and identically distributed with zero mean and a common cluster covariance matrix \(\bm{\Sigma_{b}}\).
The key part of this standard assumption is the exogeneity, namely the mean of the random effects does not depend on the fixed covariates: \(\mathbb{E}(\bm{b}_{i}|\bm{x}_{ij}:j=1,\dots,n_{i})=0\).

The configuration with a random intercept and \(q\) random slopes in a two-level hierarchy can be written as:
\begin{equation}
    \label{clmm}
    \begin{aligned}
        \eta_{ijc} = g(\gamma_{ijc}) &= \theta_{c} - (\bm{x}_{ij}^T \bm{\beta} + \bm{z}_{ij}^T \bm{b}_{i}),\qquad c=1,\dots,C-1 \\
        \bm{b}_{i} &\sim \mathcal{N}_q(\bm{0},\bm{\Sigma_{b}})
    \end{aligned}
\end{equation}
where $i=1,\dots,I$ is the level 2 (group) index and $j=1,\dots,n_{i}$ (with \(\sum_{i=1}^{I} n_{i}=J\)) is the nested level 1 index, $\gamma_{ijc} = \mathbb{P}(y_{ij} \leq c)$ is the cumulative probability up to the \textit{c}-th category for unit \textit{j} in group \textit{i}.
Fixed effects are identified by parameters \(\bm{\beta}\) associated to the entire population, while random ones are identified by group-specific parameters \(\bm{b}_{i}\).
%Since the overall intercept is fixed to zero, if it is the case of a single random intercept and no random slopes, the random effect $b_{0i}$ representing unobserved factors at the cluster level, can be seen as a random shift of the threshold parameters \(\theta_{c}\) so that the set of thresholds of cluster \(i\) is \(\theta_{c}-b_{0i}\).

In cumulative link mixed models the ordinal response variable \(y_{ij}\) with \(C\) categories is generated by a latent continuous variable \(y^*_{ij}\)  with a set of \(C-1\) thresholds \(\theta^*_{c}\) such that \(y_{ij}=c\) if and only if \(\theta^*_{c-1} \leq y^*_{ij} \leq \theta^*_{c}\).
The latent continuous variable is modelled as:
\begin{equation}
    \label{cont_clmm}
    \begin{aligned}
        y^*_{ij} = \bm{x}^{T}_{ij} \bm{\beta^*} + \bm{z}_{ij}^T \bm{b}^*_{i} + \epsilon^*_{ij}
    \end{aligned}
\end{equation}
where \(\epsilon^*_{ij}\) is a level 1 error with standard deviation \(\sigma_{\epsilon^*}\).

Therefore, the cumulative probabilities are \(\gamma_{ijc} = \mathbb{P}(y_{ij} \leq c) = \mathbb{P}(y^*_{ij} \leq \theta^*_{c}) = \mathbb{P}(\epsilon^*_{ij} \leq \theta^*_{c} - \bm{x}^{T}_{ij} \bm{\beta^*} - \bm{z}_{ij}^T \bm{b}^*_{i}) = g^{-1}(\theta_{c} - \bm{x}^{T}_{ij} \bm{\beta} - \bm{z}_{ij}^T \bm{b}_{i})\).
The underlying linear model \eqref{cont_clmm} with thresholds \(\theta^*_{c}\) and level 1 error \(\epsilon^*_{ij}\) having distribution function \(g^{-1}\) is equivalent to the cumulative model \eqref{clmm} with link function \(g\).
A generic parameter \(\alpha\) of the cumulative model can be retrieved as \(\alpha = \alpha^* \frac{\sigma_{g}}{\sigma_{\epsilon^*}}\), where \(\sigma_{g}\) is the standard deviation of the distribution associated to the link function \(g\).

As explained in \cite{grilli2011multilevel}, in a linear model like the underlying continuous model \eqref{cont_clmm} the level of unobserved heterogeneity due
to the grouping of the units is summarized by the Intraclass Correlation Coefficient (ICC)
\(\rho=\sigma^2_{b_i^*}/(\sigma^2_{b_i^*}+\sigma^2_{e^*})\). In a linear model the ICC is both the proportion of the
between-group variance with respect to the total variance and the correlation between
the responses of two units of the same group, namely \(\rho = Cor(y_{ij}^*,y_{i'j}^*|x_{ij},x_{i'j})\).
Such a correlation does not depend on the covariates, so the ICC is an
exhaustive indicator of the degree of correlation. Unfortunately, this property does not hold
in models for categorical responses such as the cumulative model in \eqref{clmm}
since \(Cor(y_{ij},y_{i'j}|x_{ij},x_{i'j})\) actually depends on the covariates. A solution is to
summarize the degree of within-group correlation using the ICC for the underlying linear
model, which can be easily computed using the random group variance \(\sigma^2_{b_i}\) of the cumulative model:
in fact, from the relationship \(\sigma_{b_i} = \sigma_{b_i^*} \frac{\sigma_{g}}{\sigma_{\epsilon^*}}\), it follows that \(\rho=\sigma^2_{b_i}/(\sigma^2_{b_i}+\sigma^2_{g})\), where \(\sigma^2_{g}\)
is the variance of the distribution associated to the link function (\(\pi^2 / 3 \cong 3.29\) for the logit link function).
% However, the ICC for the underlying linear model is misleading if one attempts to compare it with the
% values usually obtained in linear models for observed continuous responses: in fact, the
% ICC for the underlying linear model is much lower and it often gives the impression of
% a negligible within-cluster correlation. For example, a value \(\rho\) = 0,01 is negligible for an
% observed response but not for an underlying response.

In the context of CLMMs, model parameters are typically estimated using maximum likelihood methods, employing techniques such as Adaptive Gaussian Quadrature, Gauss-Hermite Quadrature, or Laplace approximation to the likelihood function.
A Newton-Raphson algorithm updates the conditional modes of the random effects for the subsequent approximations and finally a nonlinear optimization is performed over the fixed parameter set to get the Maximum Likelihood Estimation.

\subsubsection{Random forest}
\label{sec:rf}
Random forest are a powerful machine learning algorithm that combines the predictive power of multiple decision trees to make accurate and robust predictions.
In particular, random forest for regression \cite{breiman2001random,james2013introduction} are tree-based ensemble methods formed by growing regression trees such that the tree predictor \(f(\bm{x})\) takes on numerical values, and the random forest predictor is formed by taking the average over \(K\) of these trees \(f_k(\bm{x})\), \(k=1,\dots,K\).
Regression trees are an extension of classification trees, in which the dependent variable is a continuous variable, and the measure of impurity is the Mean Squared Error of the observations in the node.

As in bagging, RFs build a large number of decision trees on bootstrapped training samples, independently drawn from the distribution of the random vector (\(\bm{y}\), \(\bm{X}\)), where \(\bm{y}\) is the response variable and \(\bm{X}\) are the covariates. However, when building these decision trees, each time a split in a tree is considered, a random subsample of
covariates is chosen as split candidates among the \(p\) available ones.
Ideed, at each split the algorithm considers only a subset of the available predictors in order to avoid the case in which there is a single strong predictor which will influence all the predictions and consequently, all of the bagged trees will look quite similar to each other.
This process can be thought of as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.

Random forests are recently increasing in popularity for their ability to capture a more complex and nonlinear structure of the relationship between covariates and response. 
Moreover, their additional benefit is the possibility to assess the importance of features by using out-of-bag errors. This involves measuring the scaled difference in performance between models that include the variable of interest and those that do not.
However, a drawbacks of RFs compared to simple tree-based methods is their low level of interpretability.

\subsection{Ordinal mixed-effects random forest}
\label{sec:omerf}
The proposed statistical method, called Ordinal Mixed-Effects Random Forest (OMERF), extends the use of random forest to the analysis of hierarchical data, for categorical ordinal response variables.
It essentially models the fixed effects through a random forest, combining them to the random effects obtained using a CLMM, in order to take into account both possible complex functional forms of the covariates and the nested structure of data.
The method can be formulated as follow:
\begin{equation}
    \label{eq:omerf}
    \begin{aligned}
        \eta_{ijc}  = g(\gamma_{ijc}) &= \theta_{c} - (f(\bm{x}_{ij}) + \bm{z}_{ij}^T \bm{b}_{i})\\
        g(\gamma_{ijc}) = logi&t(\gamma_{ijc}) = ln(\frac{\gamma_{ijc}}{\gamma_{ijc}-1}) \\
        \gamma_{ijc} &= \mathbb{P}(y_{ij} \leq c) \\
        \bm{b}_{i} &\sim \mathcal{N}_q(\bm{0},\bm{\Sigma_{b}}) \\
        j=1,\dots,n_{i} \qquad i&=1,\dots,I \qquad c=1,\dots,C-1
    \end{aligned}
\end{equation}
where \(f(\bm{x}_{ij})\) is the unknown and nonlinear structure estimated through the random forest,
\(\gamma_{ijc}\) are cumulative probabilities, \(\pi_{ijc} = \mathbb{P}(y_{ij}=c) = \mathbb{P}(y_{ij} \leq c) - \mathbb{P}(y_{ij} \leq c-1) = logit^{-1}(\theta_{c} - (f(\bm{x}_{ij}) + \bm{z}_{ij}^T \bm{b}_{i})) - logit^{-1}(\theta_{c-1} - (f(\bm{x}_{ij}) + \bm{z}_{ij}^T \bm{b}_{i}))\) is the probability that the \(j\)-th observation, within the \(i\)-th group, falls in the \(c\)-th category and \(\eta_{ijc}\) is the linear predictor.
Similarly to CLMM model, OMERF model, assumes that the random effects \(\bm{b}_i\) and \(\bm{b}_{i'}\) are independent for \(i \neq i'\).
Fixed effects are estimated using a nonparametric RF model that concerns the entire population, while random effects are identified through group-specific parameters.

To implement the OMERF model, one must separate the estimation of the fixed and random effects components and alternate between them until convergence.
To achieve this, it can be observed that if the random effects are known in advance, a RF could be fitted to estimate the fixed part \(f(\bm{x}_{ij})\) by using \(\eta_{ijc}\) and \(\bm{b}_i\) as dependent variables.
Similarly, if the population-level effects are known, the random effects could be estimated using a cumulative link mixed-effects model with the response corresponding to \(\eta_{ijc}\) and \(f(\bm{x}_{ij})\).
Since neither of them is known, it is employed an iterative approach that alternates between estimating the RF for the fixed effects part and estimating the CLMM model for the random ones.
Convergence is considered achieved when the difference between the random effects estimates in two consecutive iterations is less than a predetermined tolerance.

Another significant consideration to address is the fact that \(\eta_{ijc}\) remains unknown and cannot be inferred directly from the available data.
To tackle this aspect, the approach, consistent with the methodology introduced in \cite{fontana2021performing,pellagatti2021generalized}, involves estimating \(\eta_{ijc}\) by employing a conventional CLM model, with the fixed effects covariates as predictors.
The pseudo-code outlining this estimation process is provided in Algorithm \ref{alg:alg_omerf}.

The random forest model is constructed using the R package \(randomForest\) \cite{RF}, which implements the original algorithm described in \cite{breiman2001random}. Meanwhile, the CLMM is built using the \(clmm\) function from the R package \(ordinal\) \cite{ordinal}.
The CLMM model allows for different offsets in the formula and scale effects, which are considered as components of the linear predictor that are known in advance and thus  they require no parameter to be estimated from the data. The implemented method in particular will make use of an offset modelled as:
\begin{equation}
    \label{scale_eff}
    \begin{aligned}
        \eta_{ijc}  &= g(\gamma_{ijc}) = \theta_{c} - \bm{z}_{ij}^T \bm{b}_{i} - offset_{ij}, \qquad  j=1,\dots,n_{i} \qquad i&=1,\dots,I \qquad c=1,\dots,C-1
    \end{aligned}
\end{equation}
where \(offset_{ij}\) =  \(f(\bm{x}_{ij})\), i.e. the random forest estimates of the fixed effects part.

To make predictions for a new observation \([\bm{x}_{ij};\bm{z}_{ij}]\), the following formula is employed: \(\hat{\eta}_{ijc} = \hat{\theta}_{c} - (\hat{f}(\bm{x}_{ij}) + \bm{z}_{ij}^T \hat{b}_{i})\).
Here, \(\hat{f}\) represents the random forest model estimated by the algorithm, \(\hat{b}_i\) is the vector of random effects coefficients associated with the \(i\)-th group, and \(\hat{\theta}_{c}\) is the threshold associated to the predicted category.


All the analysis are performed using R software \cite{rlanguage}. The R code for the OMERF algorithm and its auxiliary functions are available in Appendix \(\textcolor{red}{\ref{sec:appA}}\).

\subsection{Goodness of Fit indices for ordinal models}
\label{sec:gof}
The role of performance measures in predictive models is crucial, as they allow for the assessment of their Goodness of Fit and for the selection of the optimal model.
However, the choice of appropriate metrics for ordinal models is not straightforward and is an area of research still not widely explored.

Considering the accuracy for ordinal response outputs, it allows computing the percentage of observations correctly classified without taking into account their ordered scale, as it is typically used for classification models.
However, in our evaluations, we still find it useful to report this index's value. It is in fact an immediate, easy-to-understand measure that allows to actually see how many correct classifications have been made.
Given a set of \(y_{j}, \, j=1,\dots,N\) observed target values and  \(\hat{y}_{j}, \, j=1,\dots,N\) predicted target values, it is defined as:
\begin{equation}
    \label{eq:Acc}
    \begin{aligned}
        Accuracy=\frac{1}{N} \sum_{j=1}^{N} \mathbbm{1}_{y_{j}=\hat{y}_{j}} \quad \in [0,1]
    \end{aligned}
\end{equation}

Due to limitations highlighted regarding the accuracy measure, alternative metrics better suited for ordinal responses are being considered.
Firstly, we take into consideration Mean Square Error (defined in Equation \eqref{eq:MSE}), that accounts for the severity of the errors \cite{gaudette2009evaluation}. Indeed, in this type of scenario, some errors are worse than others.
A classifier which makes many small errors could be preferable to a classifier that makes fewer errors overall, but larger. For this reason, MSE can add meaningful infromation to the accuracy values.
\begin{equation}
    \label{eq:MSE}
    \begin{aligned}
        MSE=\frac{1}{N} \sum_{j=1}^{N} (y_{j}-\hat{y}_{j})^2 \quad \in [0,+\infty)
    \end{aligned}
\end{equation}

Furthermore, we incorporate the evaluation of two indexes that assess the similarity between two classifications of the same objects by quantifying the agreement proportions between the two partitions.  These two indices are the Adjusted Rand Index \cite{hubert1985comparing} and Cohen's kappa \cite{cohen1960coefficient}.
Both of these indices have a range from -1 to 1. Positive values in this range indicate agreement between the two sets, with 1 denoting perfect agreement. Negative values imply some degree of disagreement, and the magnitude of the negative value reflects the extent of this disagreement. A value equal to 0 suggests that the agreement is no different from what would be expected by chance.

Nevertheless, we wanted to compare the tested models with more task-specific performance measures. Thus, we select among the recent developements in the field the two indexes implemented by J. S. Cardoso \cite{cardoso2011measuring} and E. Ballante \cite{ballante2022new}.

Let consider the following quantities:
\begin{itemize}
    \item \(n_{r,c}, \, r,c=1,\dots,C\): number of points from the \(r\)-th class predicted as being from \(c\)-th class;
    \item \(Non-Discordant \, Pairs\): pairs of points \(x_{i}\) and \(x_{j}\) with sign(\((r_{x_{i}} - r_{x_{j}}) \times (c_{x_{i}} - c_{x_{j}}) \)) \(\leq 0\), where \(r_{x_{i}}\) and \(c_{x_{i}}\) are the row and column in the Confusion Matrix corresponding to \(x_{i}\), respectively;
    \item \(path\): sequence of entries in the Confusion Matrix where two consecutive entries in the path are 8-adjacent neighbours;
    \item \(consistent \, path\): \(path\) in which every pair of nodes in the path is non-discordant;
    \item \(Path\): set of all \(consistent \, paths\) in the Confusion Matrix from (1,1) to (C,C).
\end{itemize}
The index proposed by Cardoso is defined as: 
\begin{equation}
    \label{eq:OC}
    \begin{aligned}
        OC_{\beta}^{\gamma}= \min \left\{ 1 - \frac{\sum_{(r,c) \in Path} n_{r,c}}{N+(\sum_{\forall (r,c)} n_{r,c} |r-c|^{\gamma})^{1/\gamma}} + \beta \sum_{(r,c) \in Path} n_{r,c} |r-c|^{\gamma} \right\}
    \end{aligned}
\end{equation}
where \(OC_{\beta}^{\gamma} \in [0,1]\), and \(OC_{\beta}^{\gamma}=0\) if and only if all the elements of the vectors generating the Confusion Matrix are equal.

Whereas, having these definitions:
\begin{itemize}
    \item \(Classification \, function\): Let observations \(\left\{ 1,\dots, N\right\}\) be grouped by the estimated classes \(\hat{y}_{j}=c, \, j=1,\dots,N, c=1,\dots,C\). For each class, sort the observations in a non-increasing order with respect to \(p_{j,c}=\mathbb{P}(\hat{y}_{j}=c)\).
    The vector of indexes \(j\) of the observations is a permutation of the original vector, according to the ordering defined above. For a given model, the classification function is a piecewise constant function \(f_{mod}:[0,1] \rightarrow \left\{ 1,\dots, C\right\} \)
    such that \(f_{mod}([\frac{j-1}{N},\frac{j}{N}])=y_j, \, j=1,\dots,N\). As a special case, the \(perfect\) \(classification\) \(function\), is a piecewise constant function \(f_{exact}:[0,1] \rightarrow \left\{ 1,\dots, C\right\} \) such that each estimated class corresponds to the real
    class identified by \(\bm{y}\);
    \item \(Error \, Interval\): Consider the vector of observations ordered as described in the previous definition. Suppose that the range corresponding to the estimated class \(c\) in that vector has indexes in \([n_{c-1}, n_c)\). Let \(\widetilde{j}_c=n_{c-1},\dots, n_c\) be the index of the first misclassified observation.
    The error interval is defined as \([\frac{\widetilde{j}_c}{N},\frac{n_c}{N})\), i.e. the interval between the first misclassified observation and the last observation of the estimated class \(c\); its length is defined as \(e_c = \frac{n_c - \widetilde{j}_c}{N}\).
    If no misclassification occurs in \([n_{c-1}, n_c)\), the error interval is defined as an empty set with a length \(e_c\) = 0.
    Moreover, for each class \(c=1,\dots,C\) the corresponding weight \(w_c=\frac{e_c}{l_c}\), where \(e_c\) is the \(c\)-th error interval length and \(l_c=n_c-n_{c-1}\) is the length of the \(c\)-th estimated class in the domain, such that \(0 \leq w_c \leq 1\).
\end{itemize}
The index constructed by Ballante can be expressed as:
\begin{equation}
    \label{eq:Ball}
    \begin{aligned}
        I = \sum_{c=1}^{C} w_{c} \int_{\frac{n_{c-1}}{N}}^{\frac{n_{c}}{N}} |f_{mod}(x)-f_{exact}(x)|dx
    \end{aligned}
\end{equation}
where \(I \in [0,C-1]\), and \(I = 0\) if and only if \(f_{mod} = f_{exact}\).

Its normalized version is:
\begin{equation}
    \label{eq:Ball}
    \begin{aligned}
        I = \frac{1}{K}\sum_{c=1}^{C} w_{c} \int_{\frac{n_{c-1}}{N}}^{\frac{n_{c}}{N}} |f_{mod}(x)-f_{exact}(x)|dx
    \end{aligned}
\end{equation}
where \(K=\sum_{c=1}^{C} l_c \max \left\{ C-c,c-1\right\}\), and thus \(I \in [0,1]\).

These last two indexes are novel metrics specifically adapted to ordinal data classification problems.
As standard metrics do not adequately take into account all the information in the assessment process, these error coefficients can capture instead how much the result diverges from the
ideal prediction and how inconsistent the classifier is with regard to the relative order of the classes.

\begin{algorithm}[H]
\caption{OMERF}
\label{alg:alg_omerf}
\large
\begin{algorithmic}[1]
\STATE \textbf{Input}:
    \begin{itemize}[label={}, leftmargin=*]
        \item {$y-$} vector with ordinal categorical responses {$y_{ij}$}
        \item {$cov-$} data frame with all covariates
        \item {$group-$} vector with the grouping variable for each observation
        \item {$xnam-$} vector with names of the covariates to be used as fixed effects
        \item {$znam-$} vector with names of the covariates to be used as random effects
        \item {$b_{0}-$} optional matrix of initial values for each {$\underline{b}_{i}$}
        \item {$toll-$} threshold to decide whether our estimation converged or not
        \item {$itmax-$} maximum number of iterations
    \end{itemize}
\STATE {$Z \leftarrow$} (1;{$cov[znam]$}): it includes also the random intercept
\STATE Initialize {$b$} to a matrix of zero (if {$b_{0}$} is not given): each column {$b[i,]$} of {$b$} will be the {$i$}-th random coefficients {$\underline{b}_{i}$}
\STATE {$all.b[0]=b$}
\STATE fit a CLM model using {$y$} as response and {$cov$} as matrix of covariates
\STATE {$eta \leftarrow$} estimated {$\eta_{ijc}$} by the CLM model
\STATE {$it \leftarrow$} 1
\WHILE {$it < itmax$ \AND \NOT {$conv$}}
    \STATE {{$targ \leftarrow eta + Z \times b$}}
    \STATE fit a random forest model using {$targ$} as target and {$cov$} as predictor matrix
    \STATE {$fx \leftarrow$} fitted values of the forest model
    \STATE fit the CLMM model {$\eta_{ijc} = \theta_{c} - \underline{z}_{ij}^T \underline{b}_{i} - offset_{ij}$}, with {$offset_{ij} =  fx$}
    \STATE {$all.b[it]=b \leftarrow$} the estimated {$b$} form the model
    \STATE {$M \leftarrow$} max({$abs(b-all.b[it-1])$})
    \STATE {$(i,j) \leftarrow$} argmax({$abs(b-all.b[it-1])$})
    \STATE {$tr \leftarrow M/all.b[it-1](i,j)$}
    \IF {$tr < toll$}
        \STATE {$conv \leftarrow$} \TRUE
    \ELSE
        \STATE {$conv \leftarrow$} \FALSE
    \ENDIF
    \STATE {$it++$}
\ENDWHILE

\IF {\NOT $conv$}
    \STATE give a warning
\ENDIF
\STATE \textbf{Output}:
    \begin{itemize}[label={}, leftmargin=*]
        \item {$clmm.model-$} the final CLMM model fitted
        \item {$forest.model-$} the final forest model fitted
        \item {$b-$} the final estimation of the random coefficients
        \item {$it-$} the number of iterations
    \end{itemize}

\end{algorithmic}
\end{algorithm} 
